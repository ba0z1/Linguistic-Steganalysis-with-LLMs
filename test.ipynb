{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-10 06:42:46,407] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "from typing import List\n",
    "import argparse, logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, Dataset\n",
    "import transformers\n",
    "import json, jsonlines\n",
    "from cleantext import clean\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.96it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"/data/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348\"\n",
    "load_in_8bit = True\n",
    "device_map = \"auto\"\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            load_in_8bit=load_in_8bit,\n",
    "            device_map=device_map,\n",
    "            # cache_dir=os.path.join(args.cache_dir, \"hub\")\n",
    "        )\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "            model_name_or_path, \n",
    "            # cache_dir=os.path.join(args.cache_dir, \"hub\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): PeftModelForCausalLM(\n",
       "          (base_model): LoraModel(\n",
       "            (model): LlamaForCausalLM(\n",
       "              (model): LlamaModel(\n",
       "                (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "                (layers): ModuleList(\n",
       "                  (0-31): 32 x LlamaDecoderLayer(\n",
       "                    (self_attn): LlamaAttention(\n",
       "                      (q_proj): Linear8bitLt(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                        (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                      )\n",
       "                      (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                      (v_proj): Linear8bitLt(\n",
       "                        in_features=4096, out_features=4096, bias=False\n",
       "                        (lora_dropout): Dropout(p=0.05, inplace=False)\n",
       "                        (lora_A): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                        (lora_B): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                      )\n",
       "                      (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                      (rotary_emb): LlamaRotaryEmbedding()\n",
       "                    )\n",
       "                    (mlp): LlamaMLP(\n",
       "                      (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "                      (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "                      (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "                      (act_fn): SiLUActivation()\n",
       "                    )\n",
       "                    (input_layernorm): LlamaRMSNorm()\n",
       "                    (post_attention_layernorm): LlamaRMSNorm()\n",
       "                  )\n",
       "                )\n",
       "                (norm): LlamaRMSNorm()\n",
       "              )\n",
       "              (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = json.load(open(\"configs/TrainLM_llama-7b-hf.json\"))\n",
    "lora_hyperparams = json.load(open(\"configs/lora_config.json\"))\n",
    "target_modules = [\"query_key_value\"]\n",
    "if model_config['model_type'] == \"llama\":\n",
    "    target_modules = lora_hyperparams['lora_target_modules']  \n",
    "config = LoraConfig(\n",
    "    r=lora_hyperparams['lora_r'],\n",
    "    lora_alpha=lora_hyperparams['lora_alpha'],\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_hyperparams['lora_dropout'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "use_lora = True\n",
    "lora_weight_path = \"/data/bmh/LMforANA/mixv3/adapter_model.bin\"\n",
    "if use_lora:\n",
    "    ckpt_name = lora_weight_path\n",
    "    lora_weight = torch.load(ckpt_name)\n",
    "    set_peft_model_state_dict(model, lora_weight)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.\n",
      "\n",
      "### Question:\n",
      "Is\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"i guess if a film has magic i do n't need it to be fluid or seamless\"\n",
    "input_text = f\"### Text:\\n{input_sentence.strip()}\\n\\n### Question:\\nIs the above text steganographic or non-steganographic?\\n\\n### Answer:\\n\"\n",
    "input_text = tokenizer.bos_token + input_text if tokenizer.bos_token != None else input_text\n",
    "inputs = tokenizer([input_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "predict = model.generate(**inputs, num_beams=1, do_sample=False, max_new_tokens=10, min_new_tokens=2)\n",
    "output = tokenizer.decode(predict[0][inputs.input_ids.shape[1]:])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yjs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
